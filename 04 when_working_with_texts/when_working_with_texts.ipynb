{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# рисовалки\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# бустинги\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Dropout\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertForPreTraining\n",
    "\n",
    "from tqdm import tqdm, notebook\n",
    "notebook.tqdm.pandas()\n",
    "\n",
    "# монтируем Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "RANDOM_STATE=42\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24717e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barplot(df):\n",
    "    \"\"\"Функция для отрисовки столбчатой диаграммы с подсчетом количества наблюдений\"\"\"\n",
    "\n",
    "    counts = df['label'].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=counts.index, y=counts.values, color='#64EDC1', ec='#0B6145', alpha=0.9)\n",
    "    plt.title(f'распределение категорий в {df.name}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('категории')\n",
    "    plt.ylabel('Количество')\n",
    "    plt.tick_params(axis='x', labelsize=8.5)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(which='major', linewidth=.5)\n",
    "    plt.grid(which='minor', linewidth=.25, linestyle='--')\n",
    "\n",
    "    print('-----'*20, f'\\n{Fore.RED}{Style.BRIGHT}ДАТАСЕТ {df.name}{Style.RESET_ALL}\\n',\n",
    "          f'размер: {df.shape}\\n пропусков:{df.isna().sum().sum()}\\n явных дубликатов: {df.duplicated().sum()}\\n',\n",
    "          f'значений в id: {df.id.nunique()}, они идут по индексу равномерно? {df.index.is_monotonic_increasing}'\n",
    "    )\n",
    "    display(df.describe().T, df.head(3), df.tail(3))\n",
    "\n",
    "# get_barplot(train)\n",
    "# get_barplot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_column_and_save(df, column_name, dir_path='/'):\n",
    "    \"\"\"Удаление столбца из таблицы и сохранение его в отдельный файл\"\"\"\n",
    "\n",
    "    col_path = dir_path+f'column_{df.name}_{column_name}.csv'\n",
    "    df_path = dir_path+f'df_{df.name}.csv'\n",
    "\n",
    "    df[[column_name]].to_csv(col_path, index=False)\n",
    "    new_df = df.drop(column_name, axis=1)\n",
    "    new_df.name = df.name\n",
    "    new_df.to_csv(df_path, index=False)\n",
    "\n",
    "    print(f'столбец {column_name} сохранён в {col_path}',\n",
    "          f'датафрейм без {column_name} сохранён в {df_path}')\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def restore_column_from_file(df, column_name, file_path):\n",
    "    \"\"\"Восстановление столбца из сохраненного файла\"\"\"\n",
    "\n",
    "    id_column = pd.read_csv(file_path, index_col=0)[column_name]\n",
    "    df.insert(loc=0, column=column_name, value=id_column)\n",
    "    return df\n",
    "\n",
    "# folder_name = 'new_data/'\n",
    "# train = remove_column_and_save(train, 'id', folder_name)\n",
    "# test = remove_column_and_save(test, 'id', folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_splitter(file_name: str, chunk_size: int, header=True, get_names=False, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    функция делит один csv-файл на несколько меньших по chunk_size - количеству заданных строк\n",
    "    можно добавить заголовки и вернуть список с названием созданных файлов\n",
    "    \"\"\"\n",
    "    \n",
    "    name_files_list = []\n",
    "    df = pd.read_csv(f'{file_name}.csv', encoding=encoding)\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    count = 0\n",
    "    for i in range(num_chunks):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = start_index + chunk_size\n",
    "        chunk = df.iloc[start_index:end_index]\n",
    "        output_file = f'{file_name}_small_{i}.csv'\n",
    "        chunk.to_csv(output_file, index=False, header=header)\n",
    "        count = i + 1\n",
    "        name_files_list.append(output_file)\n",
    "    del df\n",
    "    print(f'успешный успех! Файл \"{file_name}\" разделён на {count} частей')\n",
    "    if get_names is not False:\n",
    "        return name_files_list\n",
    "\n",
    "# train_files_list = csv_splitter('new_data/labels/df_train', 5000, get_names=True)\n",
    "# print(f'TRAIN names:\\n{train_files_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d404c",
   "metadata": {},
   "source": [
    "## sbert_large_nlu_ru - https://huggingface.co/ai-forever/sbert_large_nlu_ru "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a5a48",
   "metadata": {},
   "source": [
    "вот код для получения эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb75530",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def get_embeddings(sentences):\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input.to(device))\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return sentence_embeddings.to(\"cpu\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30796d",
   "metadata": {},
   "source": [
    "ниже код, который, по сути, делает одно и то же. Разница только в том, что один его вариант собирает эмбеддинги в цикле из 22-х файлов, на которые был порезан файл train, а другой делает эмбеддинги из единого файла. Такой ход пришлось провернуть, потому как в Google Colab не загружался такой большой файл, а вот на Kaggle без проблем. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a4dfc",
   "metadata": {},
   "source": [
    "**код для Google Colab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e097f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
    "\n",
    "test = pd.read_csv(f'{path}/test.csv')\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
    "\n",
    "def process_data(df):\n",
    "    embeddings = df['text'].apply(lambda x: get_embeddings(x))\n",
    "    embeddings_array = np.squeeze(np.array(embeddings.tolist()), axis=1)\n",
    "    return embeddings_array\n",
    "\n",
    "# вот список путей к файлам датафреймов\n",
    "file_paths = [f'{path}train_small/df_train_small_{i}.csv' for i in range(22)]\n",
    "\n",
    "embeddings_list = []  # Список для хранения всех эмбеддингов\n",
    "\n",
    "for file_path in notebook.tqdm(file_paths):\n",
    "    data = pd.read_csv(file_path)\n",
    "    part_embeddings = process_data(data)  # обрабатываем текущий\n",
    "    embeddings_list.append(part_embeddings)\n",
    "    del data\n",
    "\n",
    "# вычисляем общее количество строк и создаём массив embeddings\n",
    "num_rows = sum(part_embeddings.shape[0] for part_embeddings in embeddings_list)\n",
    "embeddings = np.empty((num_rows, 1024))  # здесь можнго поиграться с размерностью эмбеддингов\n",
    "\n",
    "row_index = 0\n",
    "for part_embeddings in notebook.tqdm(embeddings_list):\n",
    "    num_rows_part = part_embeddings.shape[0]\n",
    "    embeddings[row_index : row_index + num_rows_part, :] = part_embeddings\n",
    "    row_index += num_rows_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4098eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "file_paths = [f'{path}train_small/test.csv']\n",
    "embeddings_list_test = []\n",
    "\n",
    "for file_path in notebook.tqdm(file_paths):\n",
    "    data = pd.read_csv(file_path)\n",
    "    part_embeddings = process_data(data)\n",
    "    embeddings_list_test.append(part_embeddings)\n",
    "    del data\n",
    "\n",
    "num_rows = sum(part_embeddings.shape[0] for part_embeddings in embeddings_list_test)\n",
    "embeddings_test = np.empty((num_rows, 1024))\n",
    "\n",
    "row_index = 0\n",
    "for part_embeddings in notebook.tqdm(embeddings_list_test):\n",
    "    num_rows_part = part_embeddings.shape[0]\n",
    "    embeddings_test[row_index : row_index + num_rows_part, :] = part_embeddings\n",
    "    row_index += num_rows_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = f'{path}all_embeddings'\n",
    "try:\n",
    "    os.mkdir(folder_name)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "joblib.dump(embeddings, os.path.join(folder_name, 'SBERT_embeddings_list_train'))\n",
    "joblib.dump(embeddings_test, os.path.join(folder_name, 'SBERT_embeddings_list_test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421d7b3",
   "metadata": {},
   "source": [
    "**код для Kaggle (целого файла):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d865190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/all-data/df_train.csv')\n",
    "df_test = pd.read_csv('../input/all-data/df_test.csv')\n",
    "df_train.shape, df_test.shape\n",
    "# ((105950, 2), (1952, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9af16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "y_train = df_train['label']\n",
    "df_train['text'] = df_train['text'].progress_apply(lambda x: get_embeddings(x))\n",
    "X_train = np.array(df_train['text'].tolist())\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4702d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "y_test = df_test['label']\n",
    "df_test['text'] = df_test['text'].progress_apply(lambda x: get_embeddings(x))\n",
    "X_test = np.array(df_test['text'].tolist())\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем эмбеддинги в файл\n",
    "with open(f'SBERT_embeddings_list_train_2.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open(f'y_train.pickle', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open(f'SBERT_embeddings_list_test_2.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open(f'y_test.pickle', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76da184",
   "metadata": {},
   "source": [
    "## rubert-tiny2 - https://huggingface.co/cointegrated/rubert-tiny2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e47a0",
   "metadata": {},
   "source": [
    "вот код для получения эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744063ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def embed_bert_cls(text, model=model, device=device, tokenizer=tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    #embeddings = torch.nn.functional.pad(embeddings, (0, 2048 - embeddings.shape[1])) - задать размер (до 2048)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "def filling_embeddings(df, list_):\n",
    "    for text in notebook.tqdm(df['text']):\n",
    "        list_.append(embed_bert_cls(text))\n",
    "        torch.cuda.empty_cache()  # Освобождение памяти CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd531fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list, test_embeddings_list = [], []\n",
    "\n",
    "for i in notebook.tqdm(range(22)):\n",
    "    data = pd.read_csv(f'../input/df_train_small_{i}.csv')\n",
    "    filling_embeddings(data, embeddings_list)\n",
    "    del data\n",
    "\n",
    "test_data = pd.read_csv(f'../input/test.csv')\n",
    "filling_embeddings(test_data, test_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'all_embeddings'\n",
    "try:\n",
    "    os.mkdir(folder_name)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "joblib.dump(embeddings_list, os.path.join(folder_name, 'embeddings_list_train_312'))\n",
    "joblib.dump(test_embeddings_list, os.path.join(folder_name, 'embeddings_list_test_312'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195ab1f",
   "metadata": {},
   "source": [
    "## rubert-base-cased - https://huggingface.co/DeepPavlov/rubert-base-cased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caa1a8",
   "metadata": {},
   "source": [
    "вот код для получения эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ef258",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    max_length = 512\n",
    "    # Токенизация текста и усечение до максимальной длины\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)[:max_length]\n",
    "    # Дополнение нулями до максимальной длины\n",
    "    tokens += [0] * (max_length - len(tokens))\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs[0][:, 0, :\n",
    "    del tokens, input_ids, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [f'../input/df_train_small_{i}.csv' for i in range(22)]\n",
    "embeddings_list = []\n",
    "\n",
    "for file_path in notebook.tqdm(file_paths):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['text'] = df['text'].progress_apply(lambda x: get_embeddings(x))\n",
    "    embeddings = np.array(df['text'].apply(lambda x: x.cpu().numpy()).tolist())\n",
    "    embeddings_list.append(embeddings)\n",
    "    del df\n",
    "\n",
    "X_train = np.concatenate(embeddings_list, axis=0)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['label']\n",
    "df_test['text'] = df_test['text'].progress_apply(lambda x: get_embeddings(x))\n",
    "X_test = np.array(df_test['text'].apply(lambda x: x.cpu().numpy()).tolist())\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить\n",
    "with open(f'RUBERT_BASE_CASED_embeddings_list_train.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train, f)  \n",
    "with open(f'y_train.pickle', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open(f'RUBERT_BASE_CASED_embeddings_list_test.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test, f) \n",
    "with open(f'y_test.pickle', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf944e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить\n",
    "path = '../input/embeddings/embeddings/'\n",
    "path__ = '../input/rubert-base/'\n",
    "\n",
    "X_train_r = joblib.load(os.path.join(path__, 'RUBERT_BASE_CASED_embeddings_list_train.pickle'))\n",
    "X_test_r = joblib.load(os.path.join(path__, 'RUBERT_BASE_CASED_embeddings_list_test.pickle'))\n",
    "y_train = pd.read_csv(f'{path}column_train_label.csv')['label']\n",
    "y_test = pd.read_csv(f'{path}column_test_label.csv')['label']\n",
    "X_train_r = [np.array(i) for i in X_train_r]\n",
    "X_test_r = [np.array(i) for i in X_test_r]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405000be",
   "metadata": {},
   "source": [
    "# модели - обучение, тестирование, алгоритмы на GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2bda9",
   "metadata": {},
   "source": [
    "класс, в котором формируется предсказание при помощи табличных алгоритмов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetFinalTable:\n",
    "    \"\"\"класс заполняет сводную таблицу\"\"\"\n",
    "\n",
    "    pivot_table = pd.DataFrame(columns=['модель', 'Precision на train при cv', 'Precision на test'])\n",
    "\n",
    "    @staticmethod\n",
    "    def add_result(model, precision_train='', precision_test=''):\n",
    "        GetFinalTable.pivot_table.loc[len(GetFinalTable.pivot_table)] = \\\n",
    "            [model, precision_train, precision_test]\n",
    "\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    \"\"\"класс формирует предсказание и считает Precision\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_metrics(name, model, y='', y_pred='', best_score_='', average='micro', test_df=False):\n",
    "        \"\"\"метод считает Precision\"\"\"\n",
    "\n",
    "        precision = precision_score(y, y_pred, average=average)\n",
    "        \n",
    "        if test_df is not False:\n",
    "            GetFinalTable.add_result(name, '', precision)\n",
    "        else:\n",
    "            GetFinalTable.add_result(name, best_score_, '')\n",
    "            \n",
    "        return GetFinalTable.pivot_table\n",
    "\n",
    "    \n",
    "    def choosing_best_model(self, name, model, X, y, param_grid, cv=3, n_iter=250, rs=RANDOM_STATE,\n",
    "                            X_test=False, y_test=False):\n",
    "        \"\"\"\n",
    "        метод формирует пайплайн, передаёт его RandomizedSearchCV, по сетке \n",
    "        находит лучшие гиперпараметры, возвращает предсказания\n",
    "        \"\"\"\n",
    "\n",
    "        if X_test is not False:\n",
    "            y_pred_Xtest = model.predict(X_test)\n",
    "            self.get_metrics(name, model, y=y_test, y_pred=y_pred_Xtest, test_df=True)\n",
    "            return y_pred_Xtest\n",
    "        else:\n",
    "            pipeline = Pipeline([('classifier', model)])\n",
    "            grid = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=cv,\n",
    "                                      scoring='precision_micro',\n",
    "                                      refit=True, n_iter=n_iter, random_state=rs, n_jobs=-1)\n",
    "            search = grid.fit(X, y)\n",
    "            print('---' * 10, f'\\nподобранные параметры:\\n{search.best_params_}\\n', '---' * 10)\n",
    "            pipeline.set_params(**search.best_params_)\n",
    "            pipeline.fit(X, y)\n",
    "            y_pred = pipeline.predict(X)\n",
    "            self.get_metrics(name, model, y=y, y_pred=y_pred, best_score_=search.best_score_)\n",
    "            \n",
    "            return pipeline, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для удобства сохраним название столбцов в переменную:\n",
    "del_str_train = 'Precision на train при cv'\n",
    "del_str_test = 'Precision на test'\n",
    "\n",
    "# вот экземпляр класса ModelAnalyzer:\n",
    "analyzer = ModelAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc75d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_size(X, y, test_size=0.5, random_state=RANDOM_STATE):\n",
    "    \"\"\"Функция сокращает размер эмбеддингов с сохранением распределения классов\"\"\"\n",
    "    \n",
    "    print(f'Было строк: {len(X)}\\nБыло классов: {np.unique(y).size}')\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    print(f'Стало строк: {len(X_train)}\\nСтало классов: {np.unique(y_train).size}')\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train_emb, y_train = reduce_size(X_train_emb, y_train)\n",
    "\n",
    "# Было строк: 105950\n",
    "# Было классов: 47\n",
    "# Стало строк: 52975\n",
    "# Стало классов: 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38477050",
   "metadata": {},
   "source": [
    "### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline_dict = dict()\n",
    "model = CatBoostClassifier(random_seed=RANDOM_STATE, verbose=False, loss_function='MultiClass', task_type='GPU')\n",
    "name_model = 'BERT312_CatBoostClassifier'\n",
    "params = {\n",
    "    'classifier__bagging_temperature': range(1, 9),\n",
    "    'classifier__thread_count': range(1, 6),\n",
    "    'classifier__iterations': range(501, 1501),\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "    'classifier__depth': range(1, 13),\n",
    "    'classifier__l2_leaf_reg': [0.1, 1, 3, 10, 20],\n",
    "    'classifier__border_count': [32, 64, 128, 256]\n",
    "}\n",
    "pipeline, y_pred_train =\\\n",
    "    analyzer.choosing_best_model(name_model,\n",
    "                                 model, X_train_emb, y_train, param_grid=params, cv=2, n_iter=1)\n",
    "\n",
    "pipeline_dict[name_model] = (pipeline, y_pred_train, GetFinalTable.pivot_table.iloc[[-1]])\n",
    "\n",
    "with open(f'{name_model}.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline_dict[name_model][0], f)\n",
    "\n",
    "display(GetFinalTable.pivot_table.drop(columns=del_str_test).iloc[[-1]].round(2))\n",
    "\n",
    "# ------------------------------ \n",
    "# подобранные параметры:\n",
    "# {'classifier__thread_count': 4, 'classifier__learning_rate': 0.05, 'classifier__l2_leaf_reg': 20, 'classifier__iterations': 1318, 'classifier__depth': 7, 'classifier__border_count': 64, 'classifier__bagging_temperature': 5}\n",
    "#  ------------------------------\n",
    "# CPU times: user 8min 45s, sys: 2min 3s, total: 10min 49s\n",
    "# Wall time: 8min 4s\n",
    "\n",
    "# модель\tPrecision на train при cv\n",
    "# 1\tBERT312_CatBoostClassifier\t0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a8582",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline_dict = dict()\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=47, random_state=RANDOM_STATE, tree_method='gpu_hist')\n",
    "name_model = 'BERT312_XGBClassifier'\n",
    "params = {\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 6, 9],\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__reg_lambda': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "pipeline, y_pred_train = analyzer.choosing_best_model(name_model, model, X_train_emb, y_train, \n",
    "                                                      param_grid=params, cv=2, n_iter=20)\n",
    "\n",
    "pipeline_dict[name_model] = (pipeline, y_pred_train, GetFinalTable.pivot_table.iloc[[-1]])\n",
    "\n",
    "with open(f'{name_model}.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline_dict[name_model][0], f)\n",
    "\n",
    "display(GetFinalTable.pivot_table.drop(columns=del_str_test).iloc[[-1]].round(2))\n",
    "\n",
    "# ------------------------------ \n",
    "# подобранные параметры:\n",
    "# {'classifier__reg_lambda': 10, 'classifier__n_estimators': 300, 'classifier__max_depth': 9, 'classifier__learning_rate': 0.1}\n",
    "#  ------------------------------\n",
    "# CPU times: user 12min 45s, sys: 6.52 s, total: 12min 52s\n",
    "# Wall time: 1h 11min 19s\n",
    "\n",
    "# модель\tPrecision на train при cv\n",
    "# 2\tBERT312_XGBClassifier\t0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531885b",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42031e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline_dict = dict()\n",
    "model = LGBMClassifier(objective='multiclass', num_class=47, random_state=RANDOM_STATE, device='gpu')\n",
    "name_model = 'BERT312_LGBMClassifier'\n",
    "params = {\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "    'classifier__max_depth': range(1, 13),\n",
    "    'classifier__reg_alpha': [0.1, 1, 3, 10, 20],\n",
    "    'classifier__num_boost_round': range(501, 1501)\n",
    "}\n",
    "pipeline, y_pred_train = analyzer.choosing_best_model(name_model, model, X_train_emb, y_train, param_grid=params, cv=2, n_iter=20)\n",
    "\n",
    "pipeline_dict[name_model] = (pipeline, y_pred_train, GetFinalTable.pivot_table.iloc[[-1]])\n",
    "\n",
    "with open(f'{name_model}.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline_dict[name_model][0], f)\n",
    "\n",
    "display(GetFinalTable.pivot_table.drop(columns=del_str_test).iloc[[-1]].round(2))\n",
    "\n",
    "# ------------------------------ \n",
    "# подобранные параметры:\n",
    "# {'classifier__reg_alpha': 1, 'classifier__num_boost_round': 945, 'classifier__max_depth': 2, 'classifier__learning_rate': 0.05}\n",
    "#  ------------------------------\n",
    "# CPU times: user 19min 30s, sys: 45.6 s, total: 20min 15s\n",
    "# Wall time: 2h 21min 27s\n",
    "# модель\tPrecision на train при cv\n",
    "# 3\tBERT312_LGBMClassifier\t0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка на тесте:\n",
    "\n",
    "pr = analyzer.choosing_best_model('FIN_BERT312_XGBClassifier', model_XGB,\n",
    "                                  '', '', '', X_test=X_test_emb, y_test=y_test)\n",
    "GetFinalTable.pivot_table.drop(columns='Precision на train при cv').iloc[[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc9d6d",
   "metadata": {},
   "source": [
    "## NeuralNetworkClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    \"\"\"\n",
    "    класс обучает модель на GPU, выводит прогресс обучения и отрисовывает график\n",
    "    обучения на тестовой и валидационной выборке. Обучение останавливается при \n",
    "    прохождении всех заданных эпох или если скор на валидационной выборке не менялся\n",
    "    в лучшую сторону заданное количество эпох (patience - терпение)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.1, learning_rate=0.001, patience=3):\n",
    "        \n",
    "        self.model = NeuralNet(input_size, hidden_size, output_size, dropout_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val, batch_size=32, num_epochs=10, progress_bar=1000):\n",
    "        \"\"\"обучаемся\"\"\"\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        y_train_tensor = torch.tensor(np.array(y_train), dtype=torch.long).to(self.device)\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        total_steps = len(dataloader)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (inputs, labels) in enumerate(dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Вывод шкалы прогресса\n",
    "                if (i+1) % progress_bar == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            # Оценка производительности на проверочном наборе данных\n",
    "            val_loss = self.evaluate(X_val, y_val, batch_size)\n",
    "    \n",
    "            # Проверка на улучшение\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.epochs_without_improvement = 0\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "\n",
    "            # Добавление значений ошибки в историю\n",
    "            self.val_loss_history.append(val_loss)\n",
    "            self.train_loss_history.append(loss.item())\n",
    "            \n",
    "            # Проверка на раннюю остановку\n",
    "            if self.epochs_without_improvement >= self.patience:\n",
    "                print(f'Early stopping! No improvement for {self.patience} epochs.')\n",
    "                self.plot_loss()\n",
    "                return\n",
    "            \n",
    "        self.plot_loss()\n",
    "\n",
    "            \n",
    "    def plot_loss(self):\n",
    "        \"\"\"рисум график истории обучения\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(self.train_loss_history, label='Train Loss', c='#B03A2E')\n",
    "        plt.plot(self.val_loss_history, label='Validation Loss', c='#148F77')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss', fontsize=16, fontweight='bold')\n",
    "        plt.legend(facecolor='oldlace', edgecolor='#7B6DA5')\n",
    "        plt.minorticks_on()\n",
    "        plt.grid(which='major', linewidth=.5)\n",
    "        plt.grid(which='minor', linewidth=.25, linestyle='--')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def evaluate(self, X_val, y_val, batch_size):\n",
    "        \"\"\"метод оценивает производительности модели на валидационных данных\"\"\"\n",
    "        \n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val_tensor = torch.tensor(np.array(y_val), dtype=torch.long).to(self.device)\n",
    "        dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                total_samples += inputs.size(0)\n",
    "        \n",
    "        self.model.train()\n",
    "        return total_loss / total_samples\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test, batch_size):\n",
    "        \"\"\"метод получает прогнозы модели на тестовых данных\"\"\"\n",
    "        \n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "        dataset = TensorDataset(X_test_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs in dataloader:\n",
    "                outputs = self.model(inputs[0])\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        self.model.train()\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"нейросеть сеть с несколькими скрытыми слоями. Наследуется от класса nn.Module из PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        \n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.bn_layers = nn.ModuleList()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # Создание скрытых слоев с разным количеством нейронов\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                self.hidden_layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            self.bn_layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        метод определяет прямой проход (forward pass) модели. Принимает \n",
    "        входные данные x и последовательно пропускает их через слои\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = hidden_layer(x)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = self.dropout(x) # случайный отсев, \"аналог\" резуляризации\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralNetworkClassifierL2(NeuralNetworkClassifier):\n",
    "    \"\"\" подкласс NeuralNetworkClassifier - классификатор с применением регуляризации L2\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.1, \n",
    "                 learning_rate=0.001, patience=3, weight_decay=0.001):\n",
    "        super(NeuralNetworkClassifierL2, self).__init__(input_size, hidden_size, output_size, \n",
    "                                                        dropout_rate, learning_rate, patience)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем эмбеддинги:\n",
    "path = '../input/embeddings/embeddings/'\n",
    "path_ = '../input/embeddings-list-2048/'\n",
    "\n",
    "X_train_emb = joblib.load(os.path.join(path_, 'embeddings_list_train_2048'))\n",
    "X_test_emb = joblib.load(os.path.join(path_, 'embeddings_list_test_2048'))\n",
    "y_train = pd.read_csv(f'{path}column_train_label.csv')['label']\n",
    "y_test = pd.read_csv(f'{path}column_test_label.csv')['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вот здесь слои задаём, сколько их будет всего и количество нейронов в каждом\n",
    "# потихоньку будем идти на понижение, т.к. у векторов размерность 2048, а классов 47\n",
    "hidden_sizes = [2048, 1536, 768, 384]\n",
    "\n",
    "# создаём экземпляр класса с регуляризацией L2 и явно указав шаг градиента и\n",
    "# patience - \"терпение\" - если 3 эпохи на валидационной выборке качество не будет улучшаться,\n",
    "# то, во избежание переобучения, обучение будет остановлено\n",
    "classifier = NeuralNetworkClassifierL2(input_size=2048, hidden_size=hidden_sizes, \n",
    "                                       output_size=47, learning_rate=1e-5, patience=3)\n",
    "# обучаемся и предсказываем\n",
    "classifier.train(X_train_emb, y_train, batch_size=64, num_epochs=300, X_val=X_test_emb, y_val=y_test)\n",
    "y_pred = classifier.predict(X_test_emb, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e40aa",
   "metadata": {},
   "source": [
    "![loss_0_696721.jpg](https://ltdfoto.ru/images/2023/11/01/loss_0_696721.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b98117",
   "metadata": {},
   "source": [
    "## DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4852e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(X_train, y_train, X_test, y_test, model_prediction, strategy='uniform'):\n",
    "    \"\"\"\n",
    "    Функция находит предсказание dummy-модели по заданной \n",
    "    стратегии и отрисовывает столбчатый график с подписью значений\n",
    "    \"\"\"\n",
    "\n",
    "    dummy_clf = DummyClassifier(strategy=strategy)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    dummy_pred = dummy_clf.predict(X_test)\n",
    "    dummy_precision = precision_score(y_test, dummy_pred, average='micro')\n",
    "    model_precision = 0.701332  # accuracy_score(y_test, model_prediction, average='micro')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.bar(x=[0], height=[model_precision], label='Модель', alpha=0.6, color='#48C9B0', ec='#2E6D57')\n",
    "    ax.bar(x=[1], height=[dummy_precision], label='Dummy', alpha=0.6, color='#EC7063', ec='#3167A8')\n",
    "\n",
    "    ax.text(0, model_precision, f'{model_precision:.3f}', ha='center', va='bottom')\n",
    "    ax.text(1, dummy_precision, f'{dummy_precision:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Модель', 'Dummy'])\n",
    "    plt.legend(facecolor='oldlace', edgecolor='#7B6DA5', loc='upper center')\n",
    "    plt.title(f'Сравнение предсказаний модели и dummy-модели\\nСтратегия: {strategy}',\n",
    "              fontweight='bold', fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(which='major', linewidth=.5)\n",
    "    plt.grid(which='minor', linewidth=.25, linestyle='--');\n",
    "    \n",
    "# model_comparison(X_train_emb, y_train, X_test_emb, y_test, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de4d2c",
   "metadata": {},
   "source": [
    "![dummy_bert1.jpg](https://ltdfoto.ru/images/2023/11/01/dummy_bert1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41ca67",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934386ab",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc513b5a",
   "metadata": {},
   "source": [
    "_____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
